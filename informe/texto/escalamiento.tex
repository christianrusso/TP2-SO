\section{Escalamiento a un millón de usuarios}

Si el servidor debe atender concurrentemente a un millón de clientes o más, entonces será necesario realizar modificaciones a este para solucionar los siguientes problemas:


\textbf{Hay un único proceso aceptando las conexiones de los clientes}. Como lo explicamos anteriormente, el thread principal del servidor es el encargado de recibir los pedidos de conexión de los clientes y, a partir de dichos pedidos, lanzar un thread que se encargara de atender a dicho cliente en exclusividad. Este mecanismo se puede convertir en un importante \textit{cuello de botella} del servidor cuando la cantidad de pedidos de conexión crece desmesuradamente, como en el caso de tener un millón de clientes solicitando conectarse a la vez, ya que los clientes deberán esperar mucho tiempo antes de ser atendidos. 

Para solucionar este problema se podrían utilizar varios procesos encargados de atender las conexiones de los clientes, en lugar de sólo tener un proceso. Incluso se podría ejecutar a cada uno de estos procesos en una computadora distinta y distribuir las conexiones de los clientes de forma equitativa, para así disminuir el tiempo de espera.


\textbf{Será necesario ejecutar un millón o mas de threads}. Esto es debido a que el servidor lanza un thread por cada cliente que solicite una conexión.

El primer problema que acarrea el hecho de tener tantos threads ejecutándose en el sistema esta relacionado con el scheduling de los mismos. Sin lugar a dudas, el tener que compartir el procesador con un millón de otros procesos hace que el tiempo de espera por cliente y el throughput del sistema se vean afectados. Una forma de mitigar este problema es incrementando la cantidad de núcleos de procesamiento disponibles en el servidor, ya sea con un mejor procesador o utilizando varias computadoras para distribuir el procesamiento. También sería aconsejable evaluar el desempeño del servidor con un número de clientes similar al esperado bajo distintas políticas de scheduling para así elegir la más adecuada en función de la métrica (tiempo de espera, throughput, etc) que se desee priorizar.

El segundo problema que se genera en el servidor está relacionado con la implementación de threads utilizada \textit{POSIX threads}. La cantidad máxima de threads y el tamaño por defecto de la pila de cada thread son dos parámetros muy importantes de dicha implementación.

Por un lado, la cantidad máxima de threads es un limitante directo a la cantidad de clientes que el servidor podrá atender concurrentemente, puesto que cada cliente representa un thread. En particular, en el sistema operativo Linux, el máximo número de threads permitidos (cat /proc/sys/kernel/theads-max) se define, por defecto, en función de la cantidad de memoria disponible (más memoria implica más threads) y el tamaño de la pila que se asigna a cada thread (menor tamaño de pila implica más threads), aunque este valor puede ser modificado \textit{(se necesitan permisos de root)}.
También es posible que el sistema operativo posea un límite para la cantidad de threads que un proceso puede crear, en cuyo caso, este valor también debe ser modificado de manera acorde.

Por otro lado, el tamaño de la pila de cada thread es un limitante indirecto, ya que si queremos tener un millón de threads ejecutándose y el tamaño de la pila está configurado en 2 MB (valor por defecto en sistemas de 64 bits) entonces necesitaremos 2 TB de memoria. Claramente, no es necesario poseer 2 TB de memoria RAM ya que también entra en juego la memoria virtual del sistema (swap), pero queda en evidencia la gran importancia que tiene establecer un valor correcto al tamaño de la pila de cada thread y el hecho de que la memoria disponible es un factor limitante.
Una solución obvia a estos problemas es la de instalar más memoria en el servidor o utilizar varias computadoras para distribuir los requerimientos de recursos entre cada una. Una solución mas sofisticada implicaría realizar un análisis exhaustivo del uso de la pila que realizan los threads en el sistema para así configurar de manera acorde este valor y, así, hacer un uso correcto de los recursos disponibles.

El tercer problema que se genera esta relacionado con la red que conecta al servidor con los millones de clientes. Por más grande que sea, el ancho de banda de una red es limitado, lo cual podría afectar la comunicación del servidor con los clientes. Además, los componentes de hardware que componen la red (routers, placas de red, etc) podrían ser un factor limitante en la cantidad de conexiones concurrentes que el sistema soporte. Estos problemas solo es posible solucionarlos obteniendo componentes de hardware que sean capaces de manejar los requisitos de la red del servidor.

Otro problema que se presenta es que cada conexión con un cliente ocupa un file descriptor en el servidor. Los file descriptors no son infinitos y, por lo tanto, este valor debe ser configurado de manera acorde en el sistema.

\textbf{Los datos del aula serán utilizados por un millón de clientes a la vez}. Esto se traduce en que un millón de procesos estarán compartiendo la estructura del aula en memoria generando un problema de contención de recursos y que, por ejemplo, si el aula esta casi o completamente llena entonces la cantidad de clientes (alumnos) que podrán moverse será ínfima en comparación con la cantidad total de clientes (alumnos) en el aula y, dependiendo de la política de scheduling utilizada, el tiempo que tomará que todos salgan del aula puede ser arbitrariamente grande. Esta situación no es un deadlock sino un livelock o también se la puede considerar como una situación de inanición, ya que efectivamente algunos clientes (alumnos) pueden realizar movimientos pero, por las limitaciones de espacio del aula (impuestas por el problema), muchos clientes no podrán realizar acción alguna. 
Es claro que mientras mas núcleos de procesamiento haya en el servidor más rápido se ejecutarán aquellos clientes que efectivamente pueden moverse, ya que esto aumenta el grado de paralelismo en el servidor. Otra forma de mitigar este problema (en caso de que se lo desee) podría ser implementando en el servidor un algoritmo que detecte estas situaciones, detecte a los clientes que sí pueden moverse y les otorgue una mayor prioridad frente al scheduler para así disminuir el tiempo que les tome a estos salir, dándole a los otros clientes la oportunidad de hacer algo.
